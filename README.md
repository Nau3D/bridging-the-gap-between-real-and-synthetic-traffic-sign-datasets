# Bridging the Gap Between Real and Synthetic Traffic Sign Repositories

Diogo Lopes da Silva and António Ramires Fernandes

to be published in proceedings of Delta 2022

This work aims at generating synthetic traffic sign datasets. These datasets can be be used to train CNN models that provide high levels of accuracy when tested with real data.

We employ traditional techniques for the generation of our synthetic samples, and explore two new operators: Perlin and Confetti Noise. These two operators proved essential in helping us achieve accuracies that are extremelly close to those obtained with real data datasets.

These datasets are created based only on a set of templates. We tested our synthetic datasets against three known Traffic Sign datasets:

- [GTSRB](https://benchmark.ini.rub.de/)
- [BTSC](https://btsd.ethz.ch/shareddata/)
- [rMASTIF](http://www.zemris.fer.hr/~kalfa/Datasets/rMASTIF/)

Here are some samples generated by our script for the German Traffic Sign Repository Benchmark:

![German synthetic samples](/images/gtsrb_synth.jpg)

When real data is available it is possible to add more information to the generator. In our work we explored adding brightness information. We found that all the studied datasets obbeyed a Johnson distribution regarding brightness and were able to obtain the parameters of such distribution for each data set. Based on this information we were able to generate samples with brightness values from the respective distribution.

The table below show the paramters of the Johnson distribution for each of the evaluated datasets:

| dataset | $\gamma$ | $\delta$ |  $\xi$ | $\lambda$ |
|---------|:--------:|:--------:|:------:|-----------|
| GTSRB   |   0.747  |   0.907  |  7.099 | 259.904   |
| BTSC    |   0.727  |   1.694  |  2.893 | 298.639   |
| rMASTIF |   0.664  |  $1.194  | 20.527 | 248.357   |

The following image shows the brightness distribution for the 3 datasets, and the found Johnson distribution based on the parameters of the table above.

![Johnson distribution](/images/johnson.png)

In the absence of real data, the overall brightness of the image is computed as:

$B = bias + u^\gamma \times (255 - bias)$ (Eq. 1)

where 

- $bias$ determines the minimum brightness, 
- $u$ is a sample from an uniform distribution between [0,1],
- $\gamma$ is a parameter to control the exponential.

In  our tests we set $bias=10, \gamma = 2$.

Most works so far use real scenery images as backgrounds for the synthetic samples. We also explored applying solid colour backgrounds, as shown in the image above. 

All options included we were able to generate four different datasets:

- SES: Synthetic dataset with brightness drawn from exponential equation (Eq. 1) and solid color backgrounds.
- SER: Synthetic dataset with brightness drawn from exponential equation (Eq. 1) and real image backgrounds.
- SJS: Synthetic dataset with brightness drawn from Johnson distribution and solid color backgrounds.
- SJR: Synthetic dataset with brightness drawn from Johnson distribution and real image backgrounds.

Script *generator.py* can be used to generate the synthetic datasets.

Example:

`python generator.py --templates template_location --output dest_dir --number 2000 --seed 0 --brightness exp2 --negative_folder backgrounds --negative_ratio 1`

Parameters:

- --templates: the folder where the templates are located
- --output: folder where the synthetic dataset will be written
- --number: number of samples to generate for each class
- --seed: differente seeds will provide different datasets
- --brightness: one of ['exp2', 'belgium', 'croatian', 'german']. 'exp2' refers to equation (1), whereas the other options will use the respective Johnson distribution.
- --negative_folder: (optional) if using real backgrounds, the folder where these are stored. Note: background images should be signless street views for best results.
- --negative_ratio: [0m,1] the ratio between solid color backgrounds and negative backgrounds. 0 is all solid color.

Some more options are available, see the script code.

As opposed to previous works such as [1] and [2] we didn´t aim at achieving photo-realistic imagery for our synthetic samples.  As can be seen from the samples above, our signs are not realistic at all, yet our results clearly surpass previous attempts of using synthetic datasets. This may hint that our notion of "realism" may not be the mos suitable for a CNN model.


### Refs

[1] Luo, H., Kong, Q., and Wu, F. (2018). Traffic sign image synthesis with generative adversarial networks.
In 2018 24th International Conference on Pattern Recognition (ICPR), pages 2540–2545.

[2] Spata, D., Horn, D., and Houben, S. (2019). Generation
of natural traffic sign images using domain translation with cycle-consistent generative adversarial networks. In 2019 IEEE Intelligent Vehicles Symposium
(IV), pages 702–708.

